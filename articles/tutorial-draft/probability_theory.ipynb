{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of Deep learning 01: Probability & Information Theory (draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 底空间 $\\Omega$\n",
    "\n",
    "给定一个集合 $\\Omega$，不妨取为一个“离散”集合，即 $\\Omega=\\{\\omega_1,\\cdots,\\omega_N\\}$ （元素数 cardinality， $|\\Omega|=N$），每个元素$\\omega_i$赋予的一个概率值为 $p(\\omega_i)\\equiv p_i$ （$p_i$ 满足非负性 $p_i>0$ 和归一性  $\\sum_ip_i=1$）\n",
    "\n",
    "> 对于$\\Omega$ 的任意子集 $A\\subset \\Omega$, 其对应的概率值很“自然地”定义为 $p(A) = \\sum\\limits_{\\omega\\in A} p(\\omega)$\n",
    "\n",
    "不妨把这个定义了概率值的集合$\\Omega$ 称为<span style=\"color:red\">“底空间”</span>。\n",
    "\n",
    "假设$X$ 的子集族$\\mathcal{A}=\\{A_i\\subset \\Omega\\}$满足: $\\{A_i\\}$ 彼此间的交集为空；且 $\\cup A_i=\\Omega$。则称 $\\mathcal{A}$ 是 $X$ 的一个<span style=\"color:red\">“分割”</span>\n",
    "\n",
    "\n",
    "## 2. 随机变量 $x$\n",
    "\n",
    "随机变量 $x$ 可取的“值域”（support space / events），记为  $X=\\{x_1,\\cdots,x_M\\}$ （事件集 $X$ 的 cardinality $|X|=M$）。 我们说：随机变量 $x$ <span style=\"color:red\">定义在</span> “底空间”$\\Omega$ 上；这等价于说：\n",
    "\n",
    "存在 $\\Omega$ 的某个“分割”$\\mathcal{A}$, 使得 $X$ 中的事件 与 $\\mathcal{A}$ 中的元素满足一一对应关系。例如 $x_i \\in X \\leftrightarrow A_i \\in \\mathcal{A}$, 随机变量$x$ 取 $x_i$ 值的概率定义为 $p(A_i)$\n",
    "\n",
    "\n",
    "> 之所以我们要在值域$X$ 以外还引入“底空间”的概念是因为：当我们要讨论两个随机变量的关系时，只有两个随机变量 share 同一个“底空间”，才有可能从数学上描述二者之间的关系\n",
    "\n",
    "> 由于这种一一关系，从抽象意义上来说，<span style=\"color:red\">x的值域 $X$ 就是 $\\Omega$ 的一个“分割”</span>\n",
    "\n",
    "## 3. 条件概率\n",
    "\n",
    "已知随机变量 $x, y$, 二者的support space 分别记为 $X=\\{x_i\\}, Y=\\{y_j\\}$, 二者的joint probability 函数为 $p(x_i, y_j)$ （简记为 $(X,Y)\\sim p(x, y)$）\n",
    "\n",
    "\n",
    "我们可以定义：\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x_i | y_j)\\equiv {p(x_i,y_j)\\over p(y_j)}&\\qquad\\text{conditional probablity of $x$}\\\\\n",
    "p(y_j)= \\sum_i p(x_i, y_j)&\\qquad\\text{marginal probablity of $y$}\n",
    "\\end{align}$$\n",
    "\n",
    "> 更具体来说，假设 $x_i$ 对应的“底空间”子集为 $A_i\\in\\mathcal{A}$；$y_j$ 对应的“底空间”子集为 $B_j\\in\\mathcal{B}$。 则： $$ \\begin{align}\n",
    "p(x_i)&\\equiv p(A_i)\\quad p(x_i,y_j)\\equiv p(A_i\\cap B_j)\\\\\n",
    "p(y_j)&\\equiv p(B_j)= p(\\Omega\\cap B_j)=p\\left(\\left(\n",
    "\\cup A_i \n",
    "\\right)\\cap B_j\\right)=\\sum_i p(A_i\\cap B_j)=\\sum_ip(x_i, y_j)\\\\\n",
    "p(x_i|y_j)&\\equiv {p(A_i\\cap B_j) \\over p(B_j)}\\end{align}$$\n",
    "\n",
    "\n",
    "## 4. 熵\n",
    "\n",
    "给定随机变量 $(X, Y)\\sim p(x, y)$， 定义\n",
    "\n",
    "$$\\begin{align}h(x_i)\\equiv -\\log p(x_i) &\\qquad\\text{local entropy at $x_i$}\\\\ \n",
    "h(x_i,y_j)\\equiv -\\log p(x_i,y_j)&\\qquad\\text{local joint entropy at $(x_i,y_j)$}\\\\\n",
    "h(x_i|y_j)\\equiv -\\log p(x_i|y_j)&\\qquad\\text{conditional local entropy}\\\\\n",
    "H(X) \\equiv \\sum_i p(x_i) h(x_i)=-\\sum_i p(x_i)\\log p(x_i)&\\qquad\\text{entropy of $X$}\\\\\n",
    "H(X, Y) \\equiv \\sum_{ij}p(x_i,y_j)h(x_i,y_j) = -\\sum_{ij} p(x_i,y_j)\\log p(x_i,y_j)&\\qquad\\text{joint entropy of $(X,Y)$}\\\\\n",
    "H(X|y_j)\\equiv -\\sum_i p(x_i|y_j)\\log p(x_i|y_j)&\\\\\n",
    "H(X|Y)\\equiv \\sum_j p(y_j)H(X|y_j)&\\qquad\\text{conditional entropy}\n",
    "\\end{align}$$ \n",
    "\n",
    "根据上述定义，有如下关系：\n",
    "\n",
    "$$H(X, Y)  = H(X) + H(Y|X) = H(Y) + H(X|Y)$$\n",
    "\n",
    "## 5. mutual information\n",
    "\n",
    "两个随机变量 $(X, Y)\\sim p(x, y)$ 的 mutual information 定义为：\n",
    "$$\\begin{align}\n",
    "I(X;Y) = \\sum_{ij} p(x_i,y_j)\\log{p(x_i, y_j)\\over p(x_i)p(y_j)}\n",
    "\\end{align}$$\n",
    "\n",
    "$k$ 个随机变量 $(X_1, X_2,\\cdots, X_k)\\sim p(x_1, x_2, \\cdots, x_k)$ 的 mutual information 定义为：\n",
    "\n",
    "$$\\begin{align}\n",
    "I(X_1;X_2;\\cdots;X_k) = \\sum_{x_1,x_2\\cdots x_k} p(x_1,x_2,\\cdots,x_k)\\log{p(x_1, x_2,\\cdots,x_k)\\over p(x_1)p(x_2)\\cdots p(x_k)}\n",
    "\\end{align}$$\n",
    "\n",
    "根据上述定义有：\n",
    "\n",
    "$$\\begin{align}\n",
    "I(X;Y)= H(X)- H(X|Y)=H(Y)-H(Y|X) = H(X) + H(Y) - H(X, Y)\\\\\n",
    "I(X_1;X_2;\\cdots;X_k) =  H(X_1) + H(X_2) + \\cdots + H(X_k) - H(X_1, X_2, \\cdots, X_k)\n",
    "\\end{align}$$\n",
    "\n",
    "## 6. Kullback–Leibler divergence \n",
    "\n",
    "$$D_{KL}(P‖Q) \\equiv \\sum\\limits_i p_i\\left(\\log p_i - \\log q_i\\right)$$\n",
    "    \n",
    "$D_{KL}(P‖Q)$, is the amount of information lost when Q is used to approximate P.    \n",
    "\n",
    "性质：\n",
    "\n",
    "1. 正定性：$D_{KL}(P‖Q)\\ge 0 $\n",
    "\n",
    "2. 可加性：假设$p(x, y)\\equiv p_1(x)p_2(y),\\; q(x,y)=q_1(x)q_2(y)$, 则 $D_{KL}(P‖Q)=D_{KL}(P_1‖Q_1) + D_{KL}(P_2‖Q_2)$\n",
    "\n",
    "$H(P, Q) = H(P) + D_{KL}(P‖Q)$，因此在一个最优化问题中，若所有参数集中在Q中，则minimize $D_{KL}(P‖Q)$ 等价于 minimize $H(P,Q)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes 公式\n",
    "\n",
    "给定两个随机变量 $X, Y$ 的联合概率：$p(x, y)$， 归一性关系 $\\sum\\limits_{x,y}p(x,y)=1$\n",
    "\n",
    "偏概率：$p_X(x)\\equiv\\sum\\limits_{y}p(x,y),\\;p_Y(y)\\equiv\\sum\\limits_{x}p(x,y)\\qquad$ (即$p_X$定义为将样本集$(X, Y)$中所有满足 $X=x$ 的成员的概率值求和), 归一性关系 $\\sum\\limits_{x}p_X(x) = 1$\n",
    "\n",
    "条件概率：$p(x|y)\\equiv {p(x,y)\\over p_Y(y)}$, 归一性关系 $\\sum\\limits_{x}p(x|y) = 1$\n",
    "\n",
    "根据上述定义，显然有 $$p(y|x)={p(x|y)p_Y(y)\\over p_X(x)}\\equiv{p(x|y)p_Y(y)\\over \\sum\\limits_{y^\\prime}p(x|y^\\prime)p_Y(y^\\prime)}$$\n",
    "\n",
    "该公式称为“Bayes”公式，Bayes公式从上述定义的角度来说是“平庸”的，但其意义在于对条件概率概念的“解释”和应用\n",
    "\n",
    "根据Bayes公式，显然有如下<strong>Chain Rule</strong>：\n",
    "\n",
    "$$p(x_1,x_2,\\dots,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\dots p(x_n|x_1,x_2,\\dots,x_{n-1})$$\n",
    "\n",
    "“机器学习”实际的算法模型中，由于概率密度函数一般会涉及大量的模型参数（latent variables），为简化计算，我们一般会基于一些合理假设认为，整体概率函数可以分解为若干因子的乘积（每个因子只依赖少量参数），这种乘积可以认为是对 Bayes chain rule 的简化，并可以用directional graph来的描述，例如：\n",
    "![graphic_distribution.png](./graphic_distribution.png)\n",
    "其对应关系是：\n",
    "$$p(a,b,c,d,e)=p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “自然”随机分布\n",
    "\n",
    "\n",
    "### 二项式分布(Binominal Distribution)\n",
    "\n",
    "掷硬币$N$次，硬币面朝上的几率为$\\theta$，则硬币面朝上出现次数为$n$的几率为：\n",
    "$$P_n = C_N^n\\theta^n(1-\\theta)^{N-n}$$ 称为\"二项式分布\"，记作<span style=\"color:red\">$Binominal(N, \\theta)$</span>，（$n$ 的峰值大致位于 $N\\theta$ 处）\n",
    "\n",
    "### 多项式分布(Multinominal Distribution)\n",
    "\n",
    "是二项式分布的高维推广：\n",
    "\n",
    "掷一个有$k$个面的骰子$N$次，骰子面i的几率为$\\theta_i$，各骰子面i分别出现的次数为$\\{n_i\\}$，显然 $\\sum \\theta_i=1,\\;\\sum n_i=N$，这种结果出现的几率为：\n",
    "$$P_{n_1,\\dots n_k} = {N!\\over n_1!\\dots n_k!}\\theta_1^{n_1}\\dots\\theta_k^{n_k}$$\n",
    " \n",
    "称为\"多项式分布\"，记作<span style=\"color:red\">$Multinominal(N, \\theta_1,\\dots,\\theta_k)$</span> \n",
    "\n",
    "### 泊松分布（Poisson Distribution）\n",
    "\n",
    "是二项式分布的“参数连续化”：\n",
    "\n",
    "类似于 *“全球平均每小时出生3000个小孩”* 的现象，有以下特点：\n",
    "\n",
    "1. 单个事件在自然界任意时刻的发生概率很低 （某时刻诞生*“一个小孩的概率”*远低于*“没有诞生一个小孩”*的概率）\n",
    "2. 某段时间内事件发生的次数，与且仅与这段时间的长短成正比 （全球平均每小时出生3000个，那么平均每2小时出生6000个）\n",
    "\n",
    "不妨假设，将某段时间等距划分为N个dt的时段，上帝通过“掷硬币”来决定是否在每个时段内诞生一个孩子（诞生的概率为p），显然由于特点（1），可以假设 $N\\rightarrow\\infty, p\\rightarrow0$, 这段时间内出生k个孩子的概率为二项式分布(Binorminal Distribution)：\n",
    "$$P_k = C_N^kp^k(1-p)^{N-k}$$\n",
    "\n",
    "这段时间内出生孩子个数k的期望值为：$\\langle k \\rangle = pN$, 由于特点（2），可以假设该期望值仅和这段时间的长短成正比，即 $\\langle k \\rangle ＝ pN = \\lambda t$, 其中 $t$ 表示时段的长短，$\\lambda$ 是比例常数，即 \n",
    "$$\\text{在 }N\\rightarrow \\infty, p\\rightarrow0, pN = \\lambda t\\text{ 的条件下，求 } P_k = C_N^kp^k(1-p)^{N-k}\\text{ 的极限}$$，可以证明：\n",
    "\n",
    "$$P_k = C_N^kp^k(1-p)^{N-k} \\xrightarrow{p\\rightarrow0,\\;pN=\\lambda t} P_k={(\\lambda t)^ke^{-\\lambda t}\\over k!}\\text{ (Poisson 分布)}$$\n",
    "\n",
    "其中 $\\xi\\equiv\\lambda t$ 是一个和时段长短相关的无量纲参数，并满足归一性：$\\sum\\limits_{k=0}^{\\infty}P_k = 1$, “泊松分布”记为<span style=\"color:red\">$Poisson(\\xi)$</span>，（$k$ 的峰值大致位于 $\\xi$ 处）\n",
    "\n",
    "类似于放射元素辐射的现象也和Poisson分布有密切关系，$1/\\lambda$ 衡量 “半衰期”\n",
    "\n",
    "### 正态分布（Norminal Distribution）\n",
    "\n",
    "连续随机变量$X\\in(-\\infty, +\\infty)$, 概率密度函数若满足 $$f(x)={1\\over \\sigma\\sqrt{2\\pi}}\\exp^{-{(x-\\mu)^2\\over 2\\sigma^2}}$$, 则称为“正态分布”，记为<span style=\"color:red\">$Norminal(\\mu,\\sigma)$</span> \n",
    "\n",
    "\n",
    "相关定理：\n",
    "\n",
    "二项式分布 $P_n = C_N^n\\theta^n(1-\\theta)^{N-n}$，当 $n\\theta\\gg1,\\;n(1-\\theta)\\gg1$ 时, $$Binominal(N,\\theta)\\xrightarrow{n\\theta\\gg1,\\;n(1-\\theta)\\gg1} Norminal\\left(\\mu=n\\theta,\\sigma=\\sqrt{n\\theta(1-\\theta)}\\right)$$\n",
    "\n",
    "Poisson 分布 $P_k={\\xi^ke^{-\\xi}\\over k!}$, 当 $k\\gg1$ 时，\n",
    "$$Poisson(\\xi) \\xrightarrow{k\\gg1}Norminal\\left(\\mu=\\xi,\\sigma=\\sqrt{\\xi}\\right) $$\n",
    "\n",
    "**中心极限定理**：大量统计独立的随机变量的平均值的分布趋于正态分布\n",
    "\n",
    "# 其它随机分布（一般是具有良好性质的“人工”函数，用于参数拟合等）\n",
    "\n",
    "### beta 分布\n",
    "\n",
    "对于连续随机变量$\\Theta\\in[0, 1]$，具有如下形式的概率密度函数： \n",
    "$$Beta(\\theta|a, b)\\equiv{\\theta^{a-1}(1-\\theta)^{b-1}\\over B(a,b)},\\qquad\\text{ 其中 }a>0,b>0,\\theta\\in[0,1]$$\n",
    "称为“beta分布”，其中 归一化因子 $B(a,b)\\equiv\\Gamma(a)\\Gamma(b)/\\Gamma(a+b)$，以确保 $\\int_0^1{\\rm d}\\theta Beta(\\theta|a,b)=1$\n",
    "\n",
    "$a,b>1$时，$Beta(\\theta|a, b)$ 是“钟形”曲线，峰值位于 $(a-1)/(a+b-2)$，事实上，对于 分布$Beta(\\theta|a, b)$，\n",
    "$$\\text{平均值：}E(\\theta)={a\\over a+b}\\text{,   平方差：}\\sigma_\\theta^2={ab\\over(a+b)^2(a+b+1)}$$\n",
    "\n",
    "### Dirichelet 分布\n",
    "\n",
    "是beta 分布的高维推广：\n",
    "\n",
    "假设K维随机变量 $\\{\\theta_1,\\theta_2,\\dots,\\theta_K\\}$，满足 $0\\le\\theta_i\\le1$ 且 $\\sum\\limits_i\\theta_i=1$, 具有如下形式的概率密度函数：\n",
    "$$Dir(\\theta_1,\\dots,\\theta_K|\\alpha_1,\\dots,\\alpha_K)\\equiv{1\\over B(\\alpha_1,\\dots,\\alpha_K)}\\prod\\limits_{i=1}^K\\theta_i^{\\alpha_i-1},\\qquad\\text{ 其中所有} \\alpha_i>0$$\n",
    "\n",
    "### beta分布 作为二项式分布的conjugate prior\n",
    "\n",
    "问题：试验掷硬币5次，出现3次硬币面朝上，对面朝上几率$\\theta$“最合理的”猜测值应该是多少？\n",
    "\n",
    "思路：由于此时$\\theta$ 是一个未知的待估值，可以认为$\\theta$满足某种概率分布，所谓“最合理的”猜测值，即求在某种条件概率下的期望值问题\n",
    "\n",
    "详解：掷硬币5次，所有可能出现的相关事件集为$\\{I_0, I_1, I_2, I_3, I_4, I_5\\}$, 其中$I_i$ 表示“出现i次面朝上”的事件。由于试验结果为$I_3$, 因此相关的条件概率为 $p(\\theta|I_3)$, 根据Bayes公式，有：\n",
    "$$p(\\theta|I_3)\\propto{p(I_3|\\theta)p(\\theta)}, \\text{ 其中 } p(I_3|\\theta) = C_5^3\\theta^3(1-\\theta)^2 \\text{ 是二项式分布}$$\n",
    "\n",
    "其中$p(\\theta)$ 称为$\\theta$的“先验分布”（prior），由于我们对$\\theta$\"没有任何信息量\"，因此认为 $p(\\theta)=1$，即是$[0,1]$上的均匀分布, 于是\n",
    "\n",
    "$$$p(\\theta|I_3)\\propto \\theta^3(1-\\theta)^2 \\sim Beta(\\theta|4,3)$$\n",
    "\n",
    "$Beta(\\theta|4,3)$ 的峰值在$\\theta_{\\rm peak}=3/5=0.6$，平均值$E(\\theta)=4/7 \\approx0.57$, 因此 0.57或0.6都是某种意义上的合理猜测值\n",
    "\n",
    "若我们根据某些信息用beta分布来描述 $p(\\theta)$，即$$p(\\theta)=Beta(\\theta|a,b)\\equiv\\theta^{a-1}(1-\\theta)^{b-1}/B(a,b)$$, 则\n",
    "\n",
    "$$p(\\theta|I_3)\\propto{p(I_3|\\theta)p(\\theta)} \\propto \\theta^{a+3-1}(1-\\theta)^{b+2-1}\\sim Beta(\\theta|a+3,b+2)$$\n",
    "\n",
    "(注意到：当$a=b=1$时，$Beta(\\theta|a+3,b+2)$ 还原到 $Beta(\\theta|4,3)$的结果)\n",
    "\n",
    "对比观察 posterior 概率 $p(\\theta|I_3)$ 和 $p(\\theta)$ 都满足beta函数的形式，正是因为在Bayes公式中的这种前后一致性，我们称beta分布是二项式分布的\"conjugate prior\"\n",
    "\n",
    "### Dirichelet分布 作为多项式分布的conjugate prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-distribution\n",
    "\n",
    "t-SNE: t-distributed stochastic neighbor embedding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
